{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77f66dbe-192b-471c-9cb8-e9b365e61bbb",
   "metadata": {},
   "source": [
    "# Lab | Tools prompting\n",
    "\n",
    "**Replace the existing two tools decorators, by creating 3 new ones and adjust the prompts accordingly**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b94240",
   "metadata": {},
   "source": [
    "### How to add ad-hoc tool calling capability to LLMs and Chat Models\n",
    "\n",
    ":::{.callout-caution}\n",
    "\n",
    "Some models have been fine-tuned for tool calling and provide a dedicated API for tool calling. Generally, such models are better at tool calling than non-fine-tuned models, and are recommended for use cases that require tool calling. Please see the [how to use a chat model to call tools](https://python.langchain.com/docs/how_to/tool_calling/) guide for more information.\n",
    "\n",
    "In this guide, we'll see how to add **ad-hoc** tool calling support to a chat model. This is an alternative method to invoke tools if you're using a model that does not natively support tool calling.\n",
    "\n",
    "We'll do this by simply writing a prompt that will get the model to invoke the appropriate tools. Here's a diagram of the logic:\n",
    "\n",
    "<br>\n",
    "\n",
    "![chain](https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/tool_chain.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a22cb8-19e7-450a-9d1b-6848d2c81cd1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll need to install the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c556c5e-b785-428b-8e7d-efd34a2a1adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade --quiet langchain langchain-community langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897bc01e-cc2b-4400-8a64-db4aa56085d3",
   "metadata": {},
   "source": [
    "If you'd like to use LangSmith, uncomment the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb4170-b95b-4d29-8f57-09509f3ba6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec6409b-21e5-4d0a-8a46-c4ef0b055dd3",
   "metadata": {},
   "source": [
    "You can select any of the given models for this how-to guide. Keep in mind that most of these models already [support native tool calling](https://python.langchain.com/docs/integrations/chat), so using the prompting strategy shown here doesn't make sense for these models, and instead you should follow the [how to use a chat model to call tools](https://python.langchain.com/docs/how_to/tool_calling/) guide.\n",
    "\n",
    "```{=mdx}\n",
    "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
    "\n",
    "<ChatModelTabs openaiParams={`model=\"gpt-4\"`} />\n",
    "```\n",
    "\n",
    "To illustrate the idea, we'll use `phi3` via Ollama, which does **NOT** have native support for tool calling. If you'd like to use `Ollama` as well follow [these instructions](https://python.langchain.com/docs/integrations/chat/ollama)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "424be968-2806-4d1a-a6aa-5499ae20fac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8p/lhfbj_0n501d3lhdqz4xc0dh0000gn/T/ipykernel_12176/1427064109.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  model = Ollama(model=\"phi3\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "model = Ollama(model=\"phi3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88a1463",
   "metadata": {},
   "source": [
    "\n",
    "#  How to Install and Run Ollama with the Phi-3 Model\n",
    "\n",
    "This guide walks you through installing **Ollama** and running the **Phi-3** model on Windows, macOS, and Linux.\n",
    "\n",
    "---\n",
    "\n",
    "## Windows\n",
    "\n",
    "1. **Download Ollama for Windows**  \n",
    "   Go to: [https://ollama.com/download](https://ollama.com/download)  \n",
    "   Download and run the installer.\n",
    "\n",
    "2. **Verify Installation**  \n",
    "   Open **Command Prompt** and type:\n",
    "   ```bash\n",
    "   ollama --version\n",
    "   ```\n",
    "\n",
    "3. **Run the Phi-3 Model**  \n",
    "   In the same terminal:\n",
    "   ```bash\n",
    "   ollama run phi3\n",
    "   ```\n",
    "\n",
    "4. **If you get a CUDA error (GPU memory issue)**  \n",
    "   Run Ollama in **CPU mode**:\n",
    "   ```bash\n",
    "   set OLLAMA_NO_CUDA=1\n",
    "   ollama run phi3\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "##  macOS\n",
    "\n",
    "1. **Install via Homebrew**  \n",
    "   Open the Terminal and run:\n",
    "   ```bash\n",
    "   brew install ollama\n",
    "   ```\n",
    "\n",
    "2. **Run the Phi-3 Model**\n",
    "   ```bash\n",
    "   ollama run phi3\n",
    "   ```\n",
    "\n",
    "3. **To force CPU mode (no GPU)**\n",
    "   ```bash\n",
    "   export OLLAMA_NO_CUDA=1\n",
    "   ollama run phi3\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "##  Linux\n",
    "\n",
    "1. **Install Ollama**  \n",
    "   Open a terminal and run:\n",
    "   ```bash\n",
    "   curl -fsSL https://ollama.com/install.sh | sh\n",
    "   ```\n",
    "\n",
    "2. **Run the Phi-3 Model**\n",
    "   ```bash\n",
    "   ollama run phi3\n",
    "   ```\n",
    "\n",
    "3. **To force CPU mode (no GPU)**\n",
    "   ```bash\n",
    "   export OLLAMA_NO_CUDA=1\n",
    "   ollama run phi3\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "##  Notes\n",
    "\n",
    "- The first time you run `ollama run phi3`, it will **download the model**, so make sure you’re connected to the internet.\n",
    "- Once downloaded, it works **offline**.\n",
    "- Keep the terminal open and running in the background while using Ollama from your code or notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68946881",
   "metadata": {},
   "source": [
    "## Create a tool\n",
    "\n",
    "First, let's create an `add` and `multiply` tools. For more information on creating custom tools, please see [this guide](https://python.langchain.com/docs/how_to/custom_tools/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4548e6fa-0f9b-4d7a-8fa5-66cec0350e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "Tool Name: multiply\n",
      "Description: Multiply two numbers together.\n",
      "Arguments: {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}\n",
      "--\n",
      "Tool Name: add\n",
      "Description: Add two numbers together.\n",
      "Arguments: {'x': {'title': 'X', 'type': 'integer'}, 'y': {'title': 'Y', 'type': 'integer'}}\n",
      "--\n",
      "Tool Name: subtract\n",
      "Description: Subtract y from x.\n",
      "Arguments: {'x': {'title': 'X', 'type': 'number'}, 'y': {'title': 'Y', 'type': 'number'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "# Define a tool to multiply two numbers\n",
    "@tool\n",
    "def multiply(x: float, y: float) -> float:\n",
    "    \"\"\"Multiply two numbers together.\"\"\"\n",
    "    return x * y\n",
    "\n",
    "# Define a tool to add two numbers\n",
    "@tool\n",
    "def add(x: int, y: int) -> int:\n",
    "    \"\"\"Add two numbers together.\"\"\"\n",
    "    return x + y\n",
    "\n",
    "# Define a tool to subtract two numbers\n",
    "@tool\n",
    "def subtract(x: float, y: float) -> float:\n",
    "    \"\"\"Subtract y from x.\"\"\"\n",
    "    return x - y\n",
    "\n",
    "# List of available tools\n",
    "tools = [multiply, add, subtract]\n",
    "\n",
    "# Inspect each tool's metadata\n",
    "for tool_instance in tools:\n",
    "    print(\"--\")\n",
    "    print(f\"Tool Name: {tool_instance.name}\")\n",
    "    print(f\"Description: {tool_instance.description}\")\n",
    "    print(f\"Arguments: {tool_instance.args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be77e780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply.invoke({\"x\": 4, \"y\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd690e-e54d-4209-91a4-181f69a452ac",
   "metadata": {},
   "source": [
    "## Creating our prompt\n",
    "\n",
    "We'll want to write a prompt that specifies the tools the model has access to, the arguments to those tools, and the desired output format of the model. In this case we'll instruct it to output a JSON blob of the form `{\"name\": \"...\", \"arguments\": {...}}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2063b564-25ca-4729-a45f-ba4633175b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiply(x: float, y: float) -> float - Multiply two numbers together.\n",
      "add(x: int, y: int) -> int - Add two numbers together.\n",
      "subtract(x: float, y: float) -> float - Subtract y from x.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import render_text_description\n",
    "\n",
    "rendered_tools = render_text_description(tools)\n",
    "print(rendered_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d76e1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8p/lhfbj_0n501d3lhdqz4xc0dh0000gn/T/ipykernel_12176/3399379857.py:3: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  model = ChatOllama(model=\"llama3\")\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "model = ChatOllama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56c03f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"multiply\",\n",
      "  \"arguments\": {\n",
      "    \"x\": 10,\n",
      "    \"y\": 3\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "# Render tool descriptions\n",
    "rendered_tools = \"\"\n",
    "for t in tools:\n",
    "    rendered_tools += f\"- {t.name}: {t.description}\\n  Arguments: {t.args}\\n\\n\"\n",
    "\n",
    "# System prompt with escaped curly braces\n",
    "system_prompt = \"\"\"\\\n",
    "You are an intelligent assistant that can use the following tools:\n",
    "\n",
    "{rendered_tools}\n",
    "\n",
    "Each tool has a name, a description, and a set of required arguments.\n",
    "\n",
    "Your task is to read the user's request and decide which tool to use, then return a JSON object in the following format:\n",
    "\n",
    "{{\n",
    "  \"name\": \"<tool_name>\",\n",
    "  \"arguments\": {{\n",
    "    \"<arg1>\": <value1>,\n",
    "    \"<arg2>\": <value2>\n",
    "  }}\n",
    "}}\n",
    "\n",
    "Only return this JSON. Do not include any other explanation or text.\n",
    "\"\"\"\n",
    "\n",
    "# Build the prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Load the model from Ollama\n",
    "model = ChatOllama(model=\"llama3\")  # or mistral, codellama, etc.\n",
    "\n",
    "# Chain the prompt and model\n",
    "chain = prompt | model\n",
    "\n",
    "# Invoke the chain\n",
    "message = chain.invoke({\n",
    "    \"input\": \"What's 10 times 3?\",\n",
    "    \"rendered_tools\": rendered_tools\n",
    "})\n",
    "\n",
    "# Print the result\n",
    "print(message.content if hasattr(message, 'content') else message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df2cd5-b6fa-4b10-892d-e8692c7931e5",
   "metadata": {},
   "source": [
    "## Adding an output parser\n",
    "\n",
    "We'll use the `JsonOutputParser` for parsing our models output to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f129f5bd-127c-4c95-8f34-8f437da7ca8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'multiply', 'arguments': {'x': 13, 'y': 4}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "chain = prompt | model | JsonOutputParser()\n",
    "# Invoke the chain with user input\n",
    "message = chain.invoke({\n",
    "    \"input\": \"what's thirteen times 4\",\n",
    "    \"rendered_tools\": rendered_tools\n",
    "})\n",
    "\n",
    "# Print the result (which will now be parsed as JSON)\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f08255-f146-4f4a-be43-5c21c1d3ae83",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "\n",
    "🎉 Amazing! 🎉 We now instructed our model on how to **request** that a tool be invoked.\n",
    "\n",
    "Now, let's create some logic to actually run the tool!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29dd4c-8eb5-457f-92d1-8add076404dc",
   "metadata": {},
   "source": [
    "## Invoking the tool 🏃\n",
    "\n",
    "Now that the model can request that a tool be invoked, we need to write a function that can actually invoke \n",
    "the tool.\n",
    "\n",
    "The function will select the appropriate tool by name, and pass to it the arguments chosen by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "faee95e0-4095-4310-991f-9e9465c6738e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.0\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, Optional, TypedDict\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import Tool  # Ensure you're using the correct tool class from LangChain\n",
    "\n",
    "# Define a TypedDict to represent the inputs for the invoke_tool function\n",
    "class ToolCallRequest(TypedDict):\n",
    "    \"\"\"A typed dict that shows the inputs into the invoke_tool function.\"\"\"\n",
    "    name: str\n",
    "    arguments: Dict[str, Any]\n",
    "\n",
    "# Define the function to invoke a tool by its name and arguments\n",
    "def invoke_tool(\n",
    "    tool_call_request: ToolCallRequest, config: Optional[RunnableConfig] = None\n",
    "):\n",
    "    \"\"\"Function to invoke a tool by its name and arguments.\n",
    "\n",
    "    Args:\n",
    "        tool_call_request: A dictionary containing the name and arguments of the tool to invoke.\n",
    "        config: Optional configuration for the tool invocation, like callbacks or metadata.\n",
    "\n",
    "    Returns:\n",
    "        Output from the requested tool.\n",
    "    \"\"\"\n",
    "    # Create a mapping from tool name to tool instance\n",
    "    tool_name_to_tool = {tool.name: tool for tool in tools}\n",
    "\n",
    "    # Extract the tool name from the request\n",
    "    name = tool_call_request[\"name\"]\n",
    "    \n",
    "    # Get the requested tool by its name\n",
    "    requested_tool = tool_name_to_tool.get(name)\n",
    "    if requested_tool is None:\n",
    "        raise ValueError(f\"Tool '{name}' not found.\")\n",
    "    \n",
    "    # Invoke the tool with the given arguments\n",
    "    return requested_tool.invoke(tool_call_request[\"arguments\"], config=config)\n",
    "\n",
    "# Example of how to use the invoke_tool function\n",
    "tool_call_request = {\n",
    "    \"name\": \"multiply\",  # For example, use the 'multiply' tool\n",
    "    \"arguments\": {\"x\": 13, \"y\": 4}\n",
    "}\n",
    "\n",
    "result = invoke_tool(tool_call_request)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4957532-9e0c-47f6-bb62-0fd789ac1d3e",
   "metadata": {},
   "source": [
    "Let's test this out 🧪!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0ea3b2a-8fb2-4016-83c8-a5d3e78fedbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call_request = {\n",
    "    \"name\": \"multiply\",  # Use the 'multiply' tool\n",
    "    \"arguments\": {\"x\": 13, \"y\": 4}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715af6e1-935d-4bc0-a3d2-646ecf8a329b",
   "metadata": {},
   "source": [
    "## Let's put it together\n",
    "\n",
    "Let's put it together into a chain that creates a calculator with add and multiplication capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0555b384-fde6-4404-86e0-7ea199003d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.83784653\n"
     ]
    }
   ],
   "source": [
    "# Create the chain with the prompt, model, JSON output parser, and invoke_tool\n",
    "chain = prompt | model | JsonOutputParser() | invoke_tool\n",
    "\n",
    "# Invoke the chain with user input\n",
    "message = chain.invoke({\n",
    "    \"input\": \"what's thirteen times 4.14137281\",\n",
    "    \"rendered_tools\": rendered_tools\n",
    "})\n",
    "\n",
    "# Print the result\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a9c5aa-f60a-4017-af6f-1ff6e04bfb61",
   "metadata": {},
   "source": [
    "## Returning tool inputs\n",
    "\n",
    "It can be helpful to return not only tool outputs but also tool inputs. We can easily do this with LCEL by `RunnablePassthrough.assign`-ing the tool output. This will take whatever the input is to the RunnablePassrthrough components (assumed to be a dictionary) and add a key to it while still passing through everything that's currently in the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45404406-859d-4caa-8b9d-5838162c80a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    prompt \n",
    "    | model \n",
    "    | JsonOutputParser() \n",
    "    | RunnablePassthrough.assign(output=invoke_tool)  # Pass the output to invoke_tool\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1797fe82-ea35-4cba-834a-1caf9740d184",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "This how-to guide shows the \"happy path\" when the model correctly outputs all the required tool information.\n",
    "\n",
    "In reality, if you're using more complex tools, you will start encountering errors from the model, especially for models that have not been fine tuned for tool calling and for less capable models.\n",
    "\n",
    "You will need to be prepared to add strategies to improve the output from the model; e.g.,\n",
    "\n",
    "1. Provide few shot examples.\n",
    "2. Add error handling (e.g., catch the exception and feed it back to the LLM to ask it to correct its previous output)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c02ae",
   "metadata": {},
   "source": [
    "Strategies to Improve Model Output:\n",
    "\n",
    "1. Provide Few-Shot Examples\n",
    "\t•\tWhy: Few-shot learning can help guide the model by providing it with concrete examples of what the expected output should look like. This makes it more likely to generate the correct format or invoke the correct tool.\n",
    "\t•\tHow:\n",
    "\t•\tInclude multiple examples within the prompt to show the model exactly how to handle tool invocations.\n",
    "\t•\tExample: Show it examples of different mathematical operations (e.g., addition, multiplication, etc.) with the proper JSON output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe7effa",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\\\n",
    "You are an assistant that can perform the following operations. Each tool call should be output as a JSON object with a 'name' field and an 'arguments' field.\n",
    "\n",
    "Here are a few examples:\n",
    "\n",
    "Example 1:\n",
    "Input: \"What's 5 plus 10?\"\n",
    "Output: {\"name\": \"add\", \"arguments\": {\"x\": 5, \"y\": 10}}\n",
    "\n",
    "Example 2:\n",
    "Input: \"What is 4 times 5?\"\n",
    "Output: {\"name\": \"multiply\", \"arguments\": {\"x\": 4, \"y\": 5}}\n",
    "\n",
    "Example 3:\n",
    "Input: \"What is 10 minus 3?\"\n",
    "Output: {\"name\": \"subtract\", \"arguments\": {\"x\": 10, \"y\": 3}}\n",
    "\n",
    "Now, based on the user's input, please provide the correct JSON object.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c0c1f",
   "metadata": {},
   "source": [
    "2. Add Error Handling\n",
    "\t•\tWhy: Even with the best setup, the model might still make mistakes, particularly when it’s not trained to handle tool calls directly. Error handling ensures that errors don’t halt the process and that the system can try to correct itself.\n",
    "\t•\tHow:\n",
    "\t•\tCatch the exceptions: If the model returns an unexpected result or fails to properly format the output, you can catch the error and refeed the incorrect output back into the model for correction.\n",
    "\t•\tExample of error handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10da05a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Try to invoke the chain with the user input\n",
    "    message = chain.invoke({\"input\": \"what's thirteen times 4.14137281\"})\n",
    "    print(message)\n",
    "except Exception as e:\n",
    "    # If an error occurs, prompt the model to fix the output\n",
    "    print(f\"Error occurred: {str(e)}\")\n",
    "    correction_request = \"It seems like the tool invocation output was incorrect. Please fix your output and provide the correct JSON.\"\n",
    "    corrected_message = model.invoke({\"input\": correction_request})\n",
    "    print(f\"Corrected message: {corrected_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640e6c3",
   "metadata": {},
   "source": [
    "\t•\tWhat this does: If the invoke_tool step throws an exception, the error handling section captures the error and then re-feeds the issue back to the model, asking it to correct its previous mistake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8498172b",
   "metadata": {},
   "source": [
    "Key Takeaways:\n",
    "\t•\tFew-shot examples improve model accuracy and help it generate valid tool invocations.\n",
    "\t•\tError handling ensures the system doesn’t fail when the model makes mistakes and can self-correct.\n",
    "\n",
    "    By preparing for potential issues and using these strategies, you can significantly improve the robustness of your system and ensure that it handles real-world situations more effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
